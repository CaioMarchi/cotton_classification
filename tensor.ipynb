{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, datasets, regularizers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras_tuner\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import tensorboard\n",
    "import tempfile\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações para melhorar a Pipeline de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)\n",
    "a=tf.random.normal([100,100])\n",
    "b=tf.random.normal([100,100])\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "# Verifique as GPUs disponíveis\n",
    "print(\"GPUs disponíveis:\", tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_perda, valor_precisao = model_lenet5.evaluate(test_generator)\n",
    "print('\\nPrecisão na classificação das amostras de teste:', valor_precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to c:\\users\\cmarc\\appdata\\local\\temp\\pip-req-build-s0eycgwq\n",
      "  Resolved https://github.com/tensorflow/docs to commit 3b8eeeeddb85f606812874d2aa45823227afc6bf\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting astor (from tensorflow-docs==2023.7.13.64986)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting absl-py (from tensorflow-docs==2023.7.13.64986)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting jinja2 (from tensorflow-docs==2023.7.13.64986)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting nbformat (from tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for nbformat from https://files.pythonhosted.org/packages/01/e5/322b31448ba6b0ed6de740306367e85d8da2af0d91e67f7a2860bdf87f72/nbformat-5.9.1-py3-none-any.whl.metadata\n",
      "  Downloading nbformat-5.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf>=3.12 (from tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for protobuf>=3.12 from https://files.pythonhosted.org/packages/80/70/dc63d340d27b8ff22022d7dd14b8d6d68b479a003eacdc4507150a286d9a/protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Collecting pyyaml (from tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for pyyaml from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/be/bb/08b85bc194034efbf572e70c3951549c8eca0ada25363afc154386b5390a/MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting fastjsonschema (from nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for fastjsonschema from https://files.pythonhosted.org/packages/9d/93/a3ca3cdeb84065d7d8f8df4cb09ab44405f109183c1d2b915ec17574e6b1/fastjsonschema-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading fastjsonschema-2.18.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for jsonschema>=2.6 from https://files.pythonhosted.org/packages/a1/ba/28ce987450c6afa8336373761193ddaadc1ba2004fbf23a6407db036f558/jsonschema-4.18.4-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema-4.18.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\cmarc\\onedrive\\área de trabalho\\cotton_classification\\cotton_classification\\venv\\lib\\site-packages (from nbformat->tensorflow-docs==2023.7.13.64986) (5.3.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\cmarc\\onedrive\\área de trabalho\\cotton_classification\\cotton_classification\\venv\\lib\\site-packages (from nbformat->tensorflow-docs==2023.7.13.64986) (5.9.0)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/ea/c3/f75f0ce2cdacca3d68a70b1756635092a1add1002e34afb4895b9fb62598/referencing-0.30.0-py3-none-any.whl.metadata\n",
      "  Downloading referencing-0.30.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.7.13.64986)\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/3d/ba/8639c484271d08f32b9d7044369c1aee26a29e62953be6ca4303210b4957/rpds_py-0.9.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading rpds_py-0.9.2-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\cmarc\\onedrive\\área de trabalho\\cotton_classification\\cotton_classification\\venv\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs==2023.7.13.64986) (3.9.1)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\cmarc\\onedrive\\área de trabalho\\cotton_classification\\cotton_classification\\venv\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs==2023.7.13.64986) (306)\n",
      "Using cached protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n",
      "Using cached nbformat-5.9.1-py3-none-any.whl (77 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/81.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.0/81.0 kB 1.5 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading fastjsonschema-2.18.0-py3-none-any.whl (23 kB)\n",
      "Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\n",
      "Downloading rpds_py-0.9.2-cp311-none-win_amd64.whl (180 kB)\n",
      "   ---------------------------------------- 0.0/180.7 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/180.7 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------- ------------------------- 61.4/180.7 kB 825.8 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 81.9/180.7 kB 919.0 kB/s eta 0:00:01\n",
      "   ------------------- ------------------- 92.2/180.7 kB 581.0 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 112.6/180.7 kB 504.4 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 112.6/180.7 kB 504.4 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 122.9/180.7 kB 423.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 163.8/180.7 kB 446.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 180.7/180.7 kB 454.5 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: tensorflow-docs\n",
      "  Building wheel for tensorflow-docs (setup.py): started\n",
      "  Building wheel for tensorflow-docs (setup.py): finished with status 'done'\n",
      "  Created wheel for tensorflow-docs: filename=tensorflow_docs-2023.7.13.64986-py3-none-any.whl size=185102 sha256=d1c947ceedf24783fddc29be41fd3306686fea9e70c8c0ca5571881c006ae86d\n",
      "  Stored in directory: C:\\Users\\cmarc\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-szmqabfz\\wheels\\34\\53\\89\\3db54cf97ce0f0261aaab3fdc12a847ea0879d34edf373e2c5\n",
      "Successfully built tensorflow-docs\n",
      "Installing collected packages: fastjsonschema, rpds-py, pyyaml, protobuf, MarkupSafe, attrs, astor, absl-py, referencing, jinja2, jsonschema-specifications, jsonschema, nbformat, tensorflow-docs\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astor-0.8.1 attrs-23.1.0 fastjsonschema-2.18.0 jinja2-3.1.2 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 nbformat-5.9.1 protobuf-4.23.4 pyyaml-6.0.1 referencing-0.30.0 rpds-py-0.9.2 tensorflow-docs-2023.7.13.64986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs 'C:\\Users\\cmarc\\AppData\\Local\\Temp\\pip-req-build-s0eycgwq'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc1 in position 28: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m test_data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mto_torch/test/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m valid_data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mto_torch/valid/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m train_data \u001b[39m=\u001b[39m image_dataset_from_directory(train_data_dir, batch_size\u001b[39m=\u001b[39;49mnum_batches, class_names\u001b[39m=\u001b[39;49mclasses, image_size\u001b[39m=\u001b[39;49m(image_size, image_size))\n\u001b[0;32m     14\u001b[0m valid_data \u001b[39m=\u001b[39m image_dataset_from_directory(valid_data_dir, batch_size\u001b[39m=\u001b[39mnum_batches, class_names\u001b[39m=\u001b[39m classes, image_size\u001b[39m=\u001b[39m (image_size, image_size))\n\u001b[0;32m     15\u001b[0m test_data \u001b[39m=\u001b[39m image_dataset_from_directory(test_data_dir, batch_size\u001b[39m=\u001b[39m num_batches, image_size\u001b[39m=\u001b[39m (image_size, image_size))\n",
      "File \u001b[1;32mc:\\Users\\cmarc\\OneDrive\\Área de Trabalho\\cotton_classification\\cotton_classification\\venv\\Lib\\site-packages\\keras\\src\\utils\\image_dataset.py:210\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1e6\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m image_paths, labels, class_names \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39;49mindex_directory(\n\u001b[0;32m    211\u001b[0m     directory,\n\u001b[0;32m    212\u001b[0m     labels,\n\u001b[0;32m    213\u001b[0m     formats\u001b[39m=\u001b[39;49mALLOWLIST_FORMATS,\n\u001b[0;32m    214\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[0;32m    215\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    216\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    217\u001b[0m     follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m label_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(class_names) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mWhen passing `label_mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`, there must be exactly 2 \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclass_names. Received: class_names=\u001b[39m\u001b[39m{\u001b[39;00mclass_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\cmarc\\OneDrive\\Área de Trabalho\\cotton_classification\\cotton_classification\\venv\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     subdirs \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[0;32m    543\u001b[0m         \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    544\u001b[0m             \u001b[39mif\u001b[39;00m subdir\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cmarc\\OneDrive\\Área de Trabalho\\cotton_classification\\cotton_classification\\venv\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:767\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.gfile.listdir\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    752\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist_directory_v2\u001b[39m(path):\n\u001b[0;32m    753\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[39m  The list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m    errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m         node_def\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m         op\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find directory \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m   \u001b[39m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m   \u001b[39m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cmarc\\OneDrive\\Área de Trabalho\\cotton_classification\\cotton_classification\\venv\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:689\u001b[0m, in \u001b[0;36mis_directory\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mgfile.IsDirectory\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_directory\u001b[39m(dirname):\n\u001b[0;32m    681\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[0;32m    683\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m    True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m   \u001b[39mreturn\u001b[39;00m is_directory_v2(dirname)\n",
      "File \u001b[1;32mc:\\Users\\cmarc\\OneDrive\\Área de Trabalho\\cotton_classification\\cotton_classification\\venv\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[39m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[39mreturn\u001b[39;00m _pywrap_file_io\u001b[39m.\u001b[39;49mIsDirectory(compat\u001b[39m.\u001b[39;49mpath_to_bytes(path))\n\u001b[0;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 28: invalid start byte"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "num_batches = 10\n",
    "classes = [\"classe1\",\"classe2\",\"classe3\",\"classe4\",\"classe5\"]\n",
    "image_size = 400\n",
    "\n",
    "train_data_dir = \"to_torch/train/\"\n",
    "test_data_dir = \"to_torch/test/\"\n",
    "valid_data_dir = \"to_torch/valid/\"\n",
    "\n",
    "\n",
    "train_data = image_dataset_from_directory(train_data_dir, batch_size=num_batches, class_names=classes, image_size=(image_size, image_size))\n",
    "valid_data = image_dataset_from_directory(valid_data_dir, batch_size=num_batches, class_names= classes, image_size= (image_size, image_size))\n",
    "test_data = image_dataset_from_directory(test_data_dir, batch_size= num_batches, image_size= (image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = (len(train_data) * num_batches)\n",
    "steps_per_epoch = n_train//num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a arquitetura da LeNet-5\n",
    "model_lenet5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# # Compilando o modelo\n",
    "# model_lenet5.compile(optimizer='adam', \n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "normalized_data = train_data.map(lambda x, y,: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_data))\n",
    "first_image = image_batch[0]\n",
    "\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompilerAndFit:\n",
    "    def __init__(self, optmizer, path):\n",
    "        self.optmizer = optmizer\n",
    "        self.path = path\n",
    "\n",
    "    def __get_lr_schedule(self):\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            0.001,\n",
    "            decay_steps= steps_per_epoch*100,\n",
    "            decay_rate=1,\n",
    "            staircase=False\n",
    "            )\n",
    "        return lr_schedule\n",
    "    \n",
    "    def __get_log_dir(self):\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "            print(\"Pasta criada com sucesso\")\n",
    "        else:\n",
    "            print(\"A pasta já existe\")\n",
    "        return self.path\n",
    "\n",
    "    def __get_callbacks(self):\n",
    "        return [\n",
    "            tfdocs.modeling.EpochDots(),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='acc', patience=20, mode=\"max\"),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir= self.__get_log_dir(pth)),\n",
    "        ]\n",
    "\n",
    "    def __get_optimizer(self,):\n",
    "        if self.optmizer == \"SGD\":\n",
    "            return tf.keras.optimizers.SGD(self.__get_lr_schedule)\n",
    "        elif self.optmizer == \"Adam\":\n",
    "            return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    def compile_and_fit(model, train_Data, validation_Data, max_epochs, callback_path,optimizer= None):\n",
    "        if optimizer is None:\n",
    "            optimizer = get_optimizer()\n",
    "        model.compile(optimizer= optimizer,\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                            metrics= [\n",
    "                                keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "                                keras.metrics.SparseTopKCategoricalAccuracy(2, name=\"top5-acc\")]\n",
    "                            )\n",
    "        model.summary()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_Data,\n",
    "            steps_per_epoch= steps_per_epoch,\n",
    "            validation_data= validation_Data,\n",
    "            epochs= max_epochs,\n",
    "            callbacks= get_callbacks(callback_path)\n",
    "        )\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "history[\"lenet5\"] = compile_and_fit(model_lenet5,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 100,\n",
    "                                           callback_path= \"logs_lenet5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history)\n",
    "\n",
    "val_acc = history[\"lenet5\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 - regularizers L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5_l2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.25),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.25),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = {}\n",
    "history2[\"lenet5_l2\"] = compile_and_fit(model_lenet5_l2,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 50,\n",
    "                                           callback_path= \"lenet5_l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history2)\n",
    "\n",
    "val_acc = history2[\"lenet5_l2\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 - compiler SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps= steps_per_epoch*100,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "def get_log_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"Pasta criada com sucesso\")\n",
    "    else:\n",
    "        print(\"A pasta já existe\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_callbacks(path):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='acc', patience=20, mode=\"max\"),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= get_log_dir(path)),\n",
    "    ]\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.SGD(lr_schedule)\n",
    "\n",
    "def compile_and_fit(model, train_Data, validation_Data, max_epochs, callback_path,optimizer= None):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer= optimizer,\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics= [\n",
    "                             keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "                             keras.metrics.SparseTopKCategoricalAccuracy(2, name=\"top5-acc\")]\n",
    "                         )\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_Data,\n",
    "        steps_per_epoch= steps_per_epoch,\n",
    "        validation_data= validation_Data,\n",
    "        epochs= max_epochs,\n",
    "        callbacks= get_callbacks(callback_path)\n",
    "    )\n",
    "    return history\n",
    "\n",
    "model_lenet5_l2_sgd = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = {}\n",
    "history3[\"lenet5_sgd\"] = compile_and_fit(model_lenet5_l2_sgd,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 100,\n",
    "                                           callback_path= \"logs_lenet5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history3)\n",
    "\n",
    "val_acc = history3[\"lenet5_sgd\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_generator(test_generator)\n",
    "\n",
    "get_best_hp(tuner_bayesian_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_image = model_lenet5_l2.predict(test_data)\n",
    "plt.imshow(output_image)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 - Dropouts 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps= steps_per_epoch*100,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "def get_log_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"Pasta criada com sucesso\")\n",
    "    else:\n",
    "        print(\"A pasta já existe\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_callbacks(path):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='acc', patience=20, mode=\"max\"),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= get_log_dir(path)),\n",
    "    ]\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "def compile_and_fit(model, train_Data, validation_Data, max_epochs, callback_path,optimizer= None):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer= optimizer,\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics= [\n",
    "                             keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "                             keras.metrics.SparseTopKCategoricalAccuracy(2, name=\"top5-acc\")]\n",
    "                         )\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_Data,\n",
    "        steps_per_epoch= steps_per_epoch,\n",
    "        validation_data= validation_Data,\n",
    "        epochs= max_epochs,\n",
    "        callbacks= get_callbacks(callback_path)\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definindo a arquitetura da LeNet-5\n",
    "model_lenet5_drop025 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.25),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.25),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = {}\n",
    "history4[\"lenet5_drop025\"] = compile_and_fit(model_lenet5_drop025,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 100,\n",
    "                                           callback_path= \"logs_lenet5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history4)\n",
    "\n",
    "val_acc = history4[\"lenet5_drop025\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 - Dropout 0.5 + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5_dropout_l2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu',\n",
    "                          kernel_regularizer= regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = {}\n",
    "history5[\"lenet5_drop_l2\"] = compile_and_fit(model_lenet5_dropout_l2,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 100,\n",
    "                                           callback_path= \"logs_lenet5_dropout_l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history5)\n",
    "\n",
    "val_acc = history5[\"lenet5_drop_l2\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 - (-MaxPooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5_maxpooling = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, (3, 3), activation='relu',strides= (2,2), input_shape=(image_size, image_size, 3)),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', strides=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    #TESTAR FUNÇAO SIGMOID (0)\n",
    "    tf.keras.layers.Dense(5, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history6 = {}\n",
    "history6[\"lenet5_drop_maxp\"] = compile_and_fit(model_lenet5_maxpooling,\n",
    "                                           train_Data= train_data,\n",
    "                                           validation_Data= valid_data,\n",
    "                                           max_epochs= 100,\n",
    "                                           callback_path= \"logs_lenet5_dropout_l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'acc')\n",
    "plotter.plot(history6)\n",
    "\n",
    "val_acc = history6[\"lenet5_drop_maxp\"].history['val_acc']\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(\"Maior valor de val_acc observado:\", max_val_acc)\n",
    "\n",
    "plt.axhline(max_val_acc, color='red', linestyle='dashed', label='Max val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "%tensorboard --logdir ./logs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
